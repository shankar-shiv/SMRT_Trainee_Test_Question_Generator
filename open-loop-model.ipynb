{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-07T09:17:05.036407Z","iopub.execute_input":"2022-07-07T09:17:05.036822Z","iopub.status.idle":"2022-07-07T09:17:05.064731Z","shell.execute_reply.started":"2022-07-07T09:17:05.036730Z","shell.execute_reply":"2022-07-07T09:17:05.063641Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install the latest release of Haystack in your own environment\n#! pip install farm-haystack\n\n# Install the latest master of Haystack\n!pip install --upgrade pip\n!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]\n!pip install farm-haystack[faiss]","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:17:05.081612Z","iopub.execute_input":"2022-07-07T09:17:05.081882Z","iopub.status.idle":"2022-07-07T09:18:39.226375Z","shell.execute_reply.started":"2022-07-07T09:17:05.081856Z","shell.execute_reply":"2022-07-07T09:18:39.225112Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Imports needed to run this notebook\n\nfrom pprint import pprint\nfrom tqdm import tqdm\nfrom haystack.nodes import DensePassageRetriever, BM25Retriever, QuestionGenerator, FARMReader, TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor, TransformersReader, RAGenerator\nfrom haystack.document_stores import FAISSDocumentStore\nfrom haystack.pipelines import (\n    QuestionGenerationPipeline,\n    RetrieverQuestionGenerationPipeline,\n    QuestionAnswerGenerationPipeline, # what we need\n)\nfrom haystack.utils import print_questions\n# Install these to allow pipeline visualization\n# !apt install libgraphviz-dev\n# !pip install pygraphviz","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:18:39.229806Z","iopub.execute_input":"2022-07-07T09:18:39.231035Z","iopub.status.idle":"2022-07-07T09:18:52.207708Z","shell.execute_reply.started":"2022-07-07T09:18:39.230990Z","shell.execute_reply":"2022-07-07T09:18:52.206628Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Document Store\n\n#### Option 1: FAISS\n\nFAISS is a library for efficient similarity search on a cluster of dense vectors.\nThe `FAISSDocumentStore` uses a SQL(SQLite in-memory be default) database under-the-hood\nto store the document text and other meta data. The vector embeddings of the text are\nindexed on a FAISS Index that later is queried for searching answers.\nThe default flavour of FAISSDocumentStore is \"Flat\" but can also be set to \"HNSW\" for\nfaster search at the expense of some accuracy. Just set the faiss_index_factor_str argument in the constructor.\nFor more info on which suits your use case: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index","metadata":{}},{"cell_type":"code","source":"!rm faiss_document_store.db\ndocument_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\ndocument_store","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:18:52.212747Z","iopub.execute_input":"2022-07-07T09:18:52.216614Z","iopub.status.idle":"2022-07-07T09:18:53.238893Z","shell.execute_reply.started":"2022-07-07T09:18:52.216566Z","shell.execute_reply":"2022-07-07T09:18:53.237878Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# converters exist for PDF and text files\n\nconverter = DocxToTextConverter(remove_numeric_tables=False, valid_languages=[\"en\"])\ndoc_docx = converter.convert(file_path=\"/kaggle/input/d/shankarkaggle/dataset/sanitized_manual.docx\", meta=None)[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:18:53.241687Z","iopub.execute_input":"2022-07-07T09:18:53.242488Z","iopub.status.idle":"2022-07-07T09:18:53.373214Z","shell.execute_reply.started":"2022-07-07T09:18:53.242454Z","shell.execute_reply":"2022-07-07T09:18:53.372236Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"preprocessor = PreProcessor(\n    clean_empty_lines=True,\n    clean_whitespace=True,\n    clean_header_footer=False,\n    split_by=\"word\",\n    split_length=100, # must split length for Dense Passage Retrieval\n    split_respect_sentence_boundary=True,\n)\ndocs_default = preprocessor.process([doc_docx])\nprint(\"\\n n_docs_output : {}\".format(len(docs_default)))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:18:53.374450Z","iopub.execute_input":"2022-07-07T09:18:53.374801Z","iopub.status.idle":"2022-07-07T09:18:53.453791Z","shell.execute_reply.started":"2022-07-07T09:18:53.374766Z","shell.execute_reply":"2022-07-07T09:18:53.452838Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Initialize document store and write in the documents\n# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(docs_default)\n\n# Initialize Question Generator, ask questions\nquestion_generator = QuestionGenerator()\n\n# retriever = DensePassageRetriever(\n#     document_store=document_store,\n#     query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n#     passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n#     max_seq_len_query=64,\n#     max_seq_len_passage=256,\n#     batch_size=16,\n#     use_gpu=True,\n#     embed_title=True,\n#     use_fast_tokenizers=True,\n# )\n\n# Important:\n# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n# previously indexed documents and update their embedding representation.\n# While this can be a time consuming operation (depending on corpus size), it only needs to be done once.\n# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\n# document_store.update_embeddings(retriever)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:18:53.455282Z","iopub.execute_input":"2022-07-07T09:18:53.455851Z","iopub.status.idle":"2022-07-07T09:19:31.716944Z","shell.execute_reply.started":"2022-07-07T09:18:53.455811Z","shell.execute_reply":"2022-07-07T09:19:31.715949Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Question Answer Generation Pipeline, IMPT**\n\nThis pipeline takes a document as input, generates questions on it, and attempts to answer these questions using\na Reader model","metadata":{}},{"cell_type":"code","source":"reader = FARMReader(\"deepset/roberta-base-squad2\", use_gpu=True)\ndata_dir = \"/kaggle/input/d/shankarkaggle/dataset\"\nreader.train(data_dir=data_dir, train_filename=\"answers.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")\n\n# generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\") # results were not nice, so do not use","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:19:31.718538Z","iopub.execute_input":"2022-07-07T09:19:31.718933Z","iopub.status.idle":"2022-07-07T09:22:42.599687Z","shell.execute_reply.started":"2022-07-07T09:19:31.718877Z","shell.execute_reply":"2022-07-07T09:22:42.598396Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Saving the model happens automatically at the end of training into the `save_dir` you specified\n# However, you could also save a reader manually again via:\nreader.save(directory=\"my_model\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:22:42.601949Z","iopub.execute_input":"2022-07-07T09:22:42.602364Z","iopub.status.idle":"2022-07-07T09:22:43.942387Z","shell.execute_reply.started":"2022-07-07T09:22:42.602313Z","shell.execute_reply":"2022-07-07T09:22:43.941349Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"qag_pipeline = QuestionAnswerGenerationPipeline(question_generator, reader) # Can also replace generator with reader\n\nfor idx, document in enumerate(tqdm(document_store)):\n    print(f\"\\n * Generating questions and answers for document {idx}: {document.content[:100]}...\\n\")\n    result = qag_pipeline.run(documents=[document])\n    print_questions(result)\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:22:43.943992Z","iopub.execute_input":"2022-07-07T09:22:43.944427Z","iopub.status.idle":"2022-07-07T09:25:51.619069Z","shell.execute_reply.started":"2022-07-07T09:22:43.944388Z","shell.execute_reply":"2022-07-07T09:25:51.618040Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"### To be able to make a statement about the **quality of results a question-answering pipeline** or any other pipeline in haystack produces, it is important to evaluate it. Furthermore, evaluation allows determining which components of the pipeline can be improved. The results of the evaluation can be saved as CSV files, which contain all the information to calculate additional metrics later on or inspect individual predictions","metadata":{}},{"cell_type":"markdown","source":"## Fetch, Store And Preprocess the Evaluation Dataset\n","metadata":{}},{"cell_type":"code","source":"# make sure these indices do not collide with existing ones, the indices will be wiped clean before data is inserted\ndoc_index = \"docs\"\nlabel_index = \"labels\"","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:25:51.624017Z","iopub.execute_input":"2022-07-07T09:25:51.624814Z","iopub.status.idle":"2022-07-07T09:25:51.630118Z","shell.execute_reply.started":"2022-07-07T09:25:51.624769Z","shell.execute_reply":"2022-07-07T09:25:51.628933Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Start an Elasticsearch server.\n\nYou can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source.","metadata":{}},{"cell_type":"code","source":"# If Docker is available: Start Elasticsearch as docker container\n# from haystack.utils import launch_es\n# launch_es()\n\n# Alternative in Colab / No Docker environments: Start Elasticsearch from source\n! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.9.2\n\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\n\nes_server = Popen(\n    [\"elasticsearch-7.9.2/bin/elasticsearch\"], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1)  # as daemon\n)\n# wait until ES has started\n! sleep 30","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:25:51.631559Z","iopub.execute_input":"2022-07-07T09:25:51.632031Z","iopub.status.idle":"2022-07-07T09:27:00.640921Z","shell.execute_reply.started":"2022-07-07T09:25:51.631988Z","shell.execute_reply":"2022-07-07T09:27:00.639594Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Connect to Elasticsearch\nfrom haystack.document_stores import ElasticsearchDocumentStore\n\n# Connect to Elasticsearch\ndocument_store = ElasticsearchDocumentStore(\n    host=\"localhost\",\n    username=\"\",\n    password=\"\",\n    index=doc_index,\n    label_index=label_index,\n    embedding_field=\"emb\",\n    embedding_dim=768,\n    excluded_meta_data=[\"emb\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:00.644275Z","iopub.execute_input":"2022-07-07T09:27:00.645109Z","iopub.status.idle":"2022-07-07T09:27:01.279320Z","shell.execute_reply.started":"2022-07-07T09:27:00.645075Z","shell.execute_reply":"2022-07-07T09:27:01.278334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Add evaluation data to FIASS Document Store\n# We first delete the custom tutorial indices to not have duplicate elements\n# and also split our documents into shorter passages using the PreProcessor\n\n# Add evaluation data to Elasticsearch Document Store\n# We first delete the custom tutorial indices to not have duplicate elements\n# and also split our documents into shorter passages using the PreProcessor\n\npreprocessor_eval = PreProcessor(\n    split_by=\"word\",\n    split_length=200,\n    split_overlap=0,\n    split_respect_sentence_boundary=False,\n    clean_empty_lines=False,\n    clean_whitespace=False,\n)\ndocument_store.delete_documents(index=doc_index)\ndocument_store.delete_documents(index=label_index)\n\n# The add_eval_data() method converts the given dataset in json format into Haystack document and label objects. Those objects are then indexed in their respective document and label index in the document store. The method can be used with any dataset in SQuAD format.\ndocument_store.add_eval_data(\n    filename=\"/kaggle/input/d/shankarkaggle/dataset/answers_eval.json\",\n    doc_index=doc_index,\n    label_index=label_index,\n    preprocessor=preprocessor_eval,\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:01.280879Z","iopub.execute_input":"2022-07-07T09:27:01.281247Z","iopub.status.idle":"2022-07-07T09:27:06.202028Z","shell.execute_reply.started":"2022-07-07T09:27:01.281210Z","shell.execute_reply":"2022-07-07T09:27:06.200930Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Initialize the Two Components of an ExtractiveQAPipeline: Retriever and Reader","metadata":{}},{"cell_type":"code","source":"retriever = BM25Retriever(document_store=document_store)\n\n# Alternative: Evaluate dense retrievers (EmbeddingRetriever or DensePassageRetriever)\n# The EmbeddingRetriever uses a single transformer based encoder model for query and document.\n# In contrast, DensePassageRetriever uses two separate encoders for both.\n\n# Please make sure the \"embedding_dim\" parameter in the DocumentStore above matches the output dimension of your models!\n# Please also take care that the PreProcessor splits your files into chunks that can be completely converted with\n#        the max_seq_len limitations of Transformers\n# The SentenceTransformer model \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" generally works well with the EmbeddingRetriever on any kind of English text.\n# For more information and suggestions on different models check out the documentation at: https://www.sbert.net/docs/pretrained_models.html\n\n# from haystack.retriever import EmbeddingRetriever, DensePassageRetriever\n# retriever = EmbeddingRetriever(document_store=document_store,\n#                                embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n# retriever = DensePassageRetriever(document_store=document_store,\n#                                   query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n#                                   passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n#                                   use_gpu=True,\n#                                   max_seq_len_passage=256,\n#                                   embed_title=True)\n# document_store.update_embeddings(retriever, index=doc_index)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:06.206979Z","iopub.execute_input":"2022-07-07T09:27:06.209347Z","iopub.status.idle":"2022-07-07T09:27:06.218460Z","shell.execute_reply.started":"2022-07-07T09:27:06.209304Z","shell.execute_reply":"2022-07-07T09:27:06.217407Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Initialize reader\n# reader = FARMReader(\"deepset/roberta-base-squad2\", top_k=4, return_no_answer=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:06.224472Z","iopub.execute_input":"2022-07-07T09:27:06.225969Z","iopub.status.idle":"2022-07-07T09:27:06.231137Z","shell.execute_reply.started":"2022-07-07T09:27:06.225930Z","shell.execute_reply":"2022-07-07T09:27:06.230090Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define a pipeline consisting of the initialized retriever and reader\nfrom haystack.pipelines import ExtractiveQAPipeline # \npipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)\n\n# The evaluation also works with any other pipeline.\n# For example you could use a DocumentSearchPipeline as an alternative:\n\n# from haystack.pipelines import DocumentSearchPipeline\n# pipeline = DocumentSearchPipeline(retriever=retriever)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:06.232492Z","iopub.execute_input":"2022-07-07T09:27:06.233167Z","iopub.status.idle":"2022-07-07T09:27:06.243319Z","shell.execute_reply.started":"2022-07-07T09:27:06.233126Z","shell.execute_reply":"2022-07-07T09:27:06.242266Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of an ExtractiveQAPipeline\nHere we evaluate retriever and reader in open domain fashion on the full corpus of documents i.e. a document is considered\ncorrectly retrieved if it contains the gold answer string within it. The reader is evaluated based purely on the\npredicted answer string, regardless of which document this came from and the position of the extracted span.\n\nThe generation of predictions is separated from the calculation of metrics. This allows you to run the computation-heavy model predictions only once and then iterate flexibly on the metrics or reports you want to generate.\n","metadata":{}},{"cell_type":"code","source":"from haystack.schema import EvaluationResult, MultiLabel\n\n# We can load evaluation labels from the document store\n# We are also opting to filter out no_answer samples\neval_labels = document_store.get_all_labels_aggregated(drop_negative_labels=True, drop_no_answers=True)\n\n## Alternative: Define queries and labels directly\n\n# eval_labels = [\n#    MultiLabel(\n#        labels=[\n#            Label(\n#                query=\"who is written in the book of life\",\n#                answer=Answer(\n#                    answer=\"every person who is destined for Heaven or the World to Come\",\n#                    offsets_in_context=[Span(374, 434)]\n#                ),\n#                document=Document(\n#                    id='1b090aec7dbd1af6739c4c80f8995877-0',\n#                    content_type=\"text\",\n#                    content='Book of Life - wikipedia Book of Life Jump to: navigation, search This article is\n#                       about the book mentioned in Christian and Jewish religious teachings...'\n#                ),\n#                is_correct_answer=True,\n#                is_correct_document=True,\n#                origin=\"gold-label\"\n#            )\n#        ]\n#    )\n# ]\n\n# Similar to pipeline.run() we can execute pipeline.eval()\neval_result = pipeline.eval(labels=eval_labels, params={\"Retriever\": {\"top_k\": 5}})","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:06.245331Z","iopub.execute_input":"2022-07-07T09:27:06.246796Z","iopub.status.idle":"2022-07-07T09:27:07.048060Z","shell.execute_reply.started":"2022-07-07T09:27:06.246754Z","shell.execute_reply":"2022-07-07T09:27:07.045853Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# The EvaluationResult contains a pandas dataframe for each pipeline node.\n# That's why there are two dataframes in the EvaluationResult of an ExtractiveQAPipeline.\n\nretriever_result = eval_result[\"Retriever\"]\nretriever_result.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:07.049373Z","iopub.status.idle":"2022-07-07T09:27:07.050148Z","shell.execute_reply.started":"2022-07-07T09:27:07.049850Z","shell.execute_reply":"2022-07-07T09:27:07.049877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reader_result = eval_result[\"Reader\"]\nreader_result.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:07.051456Z","iopub.status.idle":"2022-07-07T09:27:07.052164Z","shell.execute_reply.started":"2022-07-07T09:27:07.051925Z","shell.execute_reply":"2022-07-07T09:27:07.051949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can filter for all documents retrieved for a given query\nquery = \"NMS server?\"\nretriever_book_of_life = retriever_result[retriever_result[\"query\"] == query]\nretriever_book_of_life","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:07.053299Z","iopub.status.idle":"2022-07-07T09:27:07.053893Z","shell.execute_reply.started":"2022-07-07T09:27:07.053682Z","shell.execute_reply":"2022-07-07T09:27:07.053703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.print_eval_report(saved_eval_result)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:07.055279Z","iopub.status.idle":"2022-07-07T09:27:07.056005Z","shell.execute_reply.started":"2022-07-07T09:27:07.055727Z","shell.execute_reply":"2022-07-07T09:27:07.055752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Evaluation Metrics","metadata":{}},{"cell_type":"markdown","source":"Load an EvaluationResult to quickly calculate standard evaluation metrics for all predictions, such as F1-score of each individual prediction of the Reader node or recall of the retriever. To learn more about the metrics, [see Evaluation Metrics](https://haystack.deepset.ai/guides/evaluation#metrics-retrieval)","metadata":{}},{"cell_type":"code","source":"saved_eval_result = EvaluationResult.load(\"../\")\nmetrics = saved_eval_result.calculate_metrics()\nprint(f'Retriever - Recall (single relevant document): {metrics[\"Retriever\"][\"recall_single_hit\"]}')\nprint(f'Retriever - Recall (multiple relevant documents): {metrics[\"Retriever\"][\"recall_multi_hit\"]}')\nprint(f'Retriever - Mean Reciprocal Rank: {metrics[\"Retriever\"][\"mrr\"]}')\nprint(f'Retriever - Precision: {metrics[\"Retriever\"][\"precision\"]}')\nprint(f'Retriever - Mean Average Precision: {metrics[\"Retriever\"][\"map\"]}')\n\nprint(f'Reader - F1-Score: {metrics[\"Reader\"][\"f1\"]}')\nprint(f'Reader - Exact Match: {metrics[\"Reader\"][\"exact_match\"]}')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T09:27:07.057334Z","iopub.status.idle":"2022-07-07T09:27:07.058063Z","shell.execute_reply.started":"2022-07-07T09:27:07.057785Z","shell.execute_reply":"2022-07-07T09:27:07.057810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}